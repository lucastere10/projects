{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b74b21",
   "metadata": {},
   "source": [
    "# Projekt VIII: Music Genre Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e2d92a",
   "metadata": {},
   "source": [
    "As simple as it gets, make a Music Genre Prediction based on the data given.\n",
    "\n",
    "            key features: EDA, Supervised Learning, Logistic Regression, KNN, Decision Tree, GradientBoostingClassifier\n",
    "\n",
    "#### Objectives:\n",
    "\n",
    "   * Music Genre Prediction\n",
    "   \n",
    "Target Feature: quality\n",
    "\n",
    "##### Used Algorithims\n",
    "\n",
    "- KNeighborsClassifier\n",
    "- DecisionTreeClassifier\n",
    "\n",
    "#### Phases:\n",
    "   * Data Preparation\n",
    "   * Data Analysis\n",
    "   * Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53dd228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%javascript` not found.\n"
     ]
    }
   ],
   "source": [
    "#lib imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import cufflinks as cf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plotly\n",
    "import chart_studio.plotly as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "#Scikit and Yellowbrick\n",
    "from sklearn.metrics import classification_report, confusion_matrix, r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier , plot_tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from yellowbrick.cluster import KElbowVisualizer #elbow method\n",
    "\n",
    "#pycaret\n",
    "#from pycaret.classification import *\n",
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframe\n",
    "df = pd.read_csv('data/train.csv')\n",
    "genre = pd.read_csv('data/submission.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f158e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c37268",
   "metadata": {},
   "source": [
    "Normal procedure: head, info, describe, look for NaNs and Nulls, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05929595",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d96aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909c899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a7dfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NaN values visualization\n",
    "fix, (ax1,ax2) = plt.subplots(1,2,figsize=(12, 8), dpi=75)\n",
    "sns.heatmap(data = df.isnull(), cmap=\"PuBu_r\", ax = ax1).set_title('Before')\n",
    "\n",
    "#loads of NaNs, lets work on it\n",
    "cols = ['Popularity','key','instrumentalness']\n",
    "for i in cols:\n",
    "    df[i].fillna(df[i].mean(), inplace = True)\n",
    "    \n",
    "#NaN values after mean fill\n",
    "sns.heatmap(data = df.isnull(), cmap=\"PuBu_r\", ax = ax2).set_title('After')\n",
    "\n",
    "#change columns name\n",
    "df.rename(columns = {'duration_in min/ms':'duration'}, inplace = True)\n",
    "test.rename(columns = {'duration_in min/ms':'duration'}, inplace = True)\n",
    "#remove song titles\n",
    "df.drop('Track Name', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80152aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets deal with categorical variables\n",
    "\n",
    "#backup data \n",
    "backup = df\n",
    "#Get list of categorical variables\n",
    "s = (df.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "print(object_cols)\n",
    "\n",
    "#Transform Artist name in numerical value\n",
    "LE = LabelEncoder()\n",
    "df['Artist Name'] = df[['Artist Name']].apply(LE.fit_transform)\n",
    "test['Artist Name'] = test[['Artist Name']].apply(LE.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63104e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_corr = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(df_corr, annot = True, center = 0, cmap = 'twilight_shifted')\n",
    "ax.set_title('Correlation Matrix', fontsize = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88275475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()['Class'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8e0d4e",
   "metadata": {},
   "source": [
    "No realy high correlation can be seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f85460",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Class Distribution\n",
    "df_count = df['Class'].value_counts().reset_index().sort_values(by = 'index', ascending = False).rename(columns={'Class':'Count','index':'Class'})\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=75)\n",
    "sns.countplot(data = df, x = 'Class', palette = 'twilight_shifted_r')\n",
    "\n",
    "#title & axis\n",
    "xmin, xmax = ax.get_xlim()\n",
    "ymin, ymax = ax.get_ylim()\n",
    "ax.set_title('Class Distribution',\n",
    "             fontdict = {'fontsize': 20},\n",
    "            loc = 'left')\n",
    "ax.set_ylabel('')\n",
    "ax.grid(axis = 'y',linestyle = 'dotted')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.set_ylim(0,ymax*1.1)\n",
    "\n",
    "#xtick values\n",
    "rate_list = list(range(0,11))\n",
    "ticks = df_count.sort_values(by = 'Class', ascending = True).reset_index()\n",
    "for i in rate_list:\n",
    "    ax.text(x = i, y = ticks['Count'][i],\n",
    "            s = ticks['Count'][i],\n",
    "            horizontalalignment= 'center', verticalalignment = 'bottom', \n",
    "            fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab20cee",
   "metadata": {},
   "source": [
    "Will be necessary to balance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f177023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all histograms\n",
    "df.hist(bins=50, figsize=(20,15),color='green',alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c9c67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#verify outliers\n",
    "fig, [(ax1, ax2, ax3),(ax4, ax5, ax6),(ax7, ax8, ax9), (ax10,ax11, ax12)] = plt.subplots(nrows = 4, ncols = 3, figsize = (16 , 30))\n",
    "fig.suptitle('Verify Outliers', fontsize = 24,y = 0.9)\n",
    "\n",
    "#first row\n",
    "sns.boxplot(data = df, x = 'Class', y = 'Popularity', palette= 'twilight_shifted', ax = ax1)\n",
    "sns.boxplot(data = df, x = 'Class', y = 'danceability', palette= 'twilight_shifted', ax = ax2)\n",
    "sns.boxplot(data = df, x = 'Class', y = 'energy', palette= 'twilight_shifted', ax = ax3)\n",
    "#second row\n",
    "sns.boxplot(data = df, x = 'Class', y = 'loudness', palette= 'twilight_shifted', ax = ax4)\n",
    "sns.boxplot(data = df, x = 'Class', y = 'speechiness', palette= 'twilight_shifted', ax = ax5)\n",
    "sns.boxplot(data = df, x = 'Class', y = 'acousticness', palette= 'twilight_shifted', ax = ax6)\n",
    "#third row\n",
    "sns.boxplot(data = df, x = 'Class', y = 'instrumentalness', palette= 'twilight_shifted', ax = ax7)\n",
    "sns.boxplot(data = df, x = 'Class', y = 'liveness', palette= 'twilight_shifted', ax = ax8)\n",
    "sns.boxplot(data = df, x = 'Class', y = 'valence', palette= 'twilight_shifted', ax = ax9)\n",
    "#fourth row\n",
    "sns.boxplot(data = df, x = 'Class', y = 'tempo', palette= 'twilight_shifted', ax = ax10)\n",
    "sns.boxplot(data = df, x = 'Class', y = 'duration', palette= 'twilight_shifted', ax = ax11)\n",
    "sns.countplot(data = df, x = 'Class', palette = 'twilight_shifted_r', ax = ax12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf1a04",
   "metadata": {},
   "source": [
    "Good portion of atributes have really high number of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2712fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew(numeric_only=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc04100",
   "metadata": {},
   "source": [
    "#### Data conclusion\n",
    "\n",
    "* Huge number of outliers\n",
    "* Some atributes are not normalized\n",
    "* No atribute with big correlation with quality\n",
    "* Highly Skewed data\n",
    "* Some quantity gap between Class values\n",
    "* Normalize columns: [energy;loudness; speechiness; acousticness; instrumentalness; liveness; duration]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d395b6",
   "metadata": {},
   "source": [
    "#### Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd3b25",
   "metadata": {},
   "source": [
    "Since we have 0 values we can't use log(x) transformation. A good way to fix it is use a constant like log(x+1). Since we also have negative values, the maximum negative value must be added. We'ill use log(x + min +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592868ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimum value\n",
    "df['loudness'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c16fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make normalized df\n",
    "df_normalized = df\n",
    "\n",
    "def log_transform(col):\n",
    "    return np.log(col[0] + 39.952 + 1)\n",
    "\n",
    "df_normalized['energy'] = df[['energy']].applymap(lambda x: np.log(x + 39.952 + 1))\n",
    "df_normalized['loudness'] = df[['loudness']].applymap(lambda x: np.log(x + 39.952 + 1))\n",
    "df_normalized['speechiness'] = df[['speechiness']].applymap(lambda x: np.log(x + 39.952 + 1))\n",
    "df_normalized['acousticness'] = df[['acousticness']].applymap(lambda x: np.log(x + 39.952 + 1))\n",
    "df_normalized['instrumentalness'] = df[['instrumentalness']].applymap(lambda x: np.log(x + 39.952 + 1))\n",
    "df_normalized['liveness'] = df[['liveness']].applymap(lambda x: np.log(x + 39.952 + 1))\n",
    "df_normalized['duration'] = df[['duration']].applymap(lambda x: np.log(x + 39.952 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bdfafc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#all histograms\n",
    "df_normalized.hist(bins=50, figsize=(20,15),color='green',alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd724fd",
   "metadata": {},
   "source": [
    "#### Oversample and Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b0e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'].value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dfs\n",
    "df_0 = df_normalized[df_normalized.Class==0]\n",
    "df_1 = df_normalized[df_normalized.Class==1]\n",
    "df_2 = df_normalized[df_normalized.Class==2]\n",
    "df_3 = df_normalized[df_normalized.Class==3]         \n",
    "df_4 = df_normalized[df_normalized.Class==4]         \n",
    "df_5 = df_normalized[df_normalized.Class==5]\n",
    "df_6 = df_normalized[df_normalized.Class==6]\n",
    "df_7 = df_normalized[df_normalized.Class==7]\n",
    "df_8 = df_normalized[df_normalized.Class==8]\n",
    "df_8 = df_normalized[df_normalized.Class==9]\n",
    "df_8 = df_normalized[df_normalized.Class==10]\n",
    "\n",
    "# oversample:\n",
    "df_0_upsampled = resample(df_0, replace=True, n_samples=1636, random_state=12) \n",
    "df_1_upsampled = resample(df_1, replace=True, n_samples=1636, random_state=12) \n",
    "df_2_upsampled = resample(df_2, replace=True, n_samples=1636, random_state=12) \n",
    "df_3_upsampled = resample(df_3, replace=True, n_samples=1636, random_state=12)\n",
    "df_4_upsampled = resample(df_4, replace=True, n_samples=1636, random_state=12)\n",
    "df_5_upsampled = resample(df_5, replace=True, n_samples=1636, random_state=12)\n",
    "df_7_upsampled = resample(df_7, replace=True, n_samples=1636, random_state=12)\n",
    "\n",
    "# downsample:\n",
    "df_6_downsampled = df_normalized[df_normalized.Class==6].sample(n=1636).reset_index(drop=True)\n",
    "df_8_downsampled = df_normalized[df_normalized.Class==8].sample(n=1636).reset_index(drop=True)\n",
    "df_9_downsampled = df_normalized[df_normalized.Class==9].sample(n=1636).reset_index(drop=True)\n",
    "df_10_downsampled = df_normalized[df_normalized.Class==10].sample(n=1636).reset_index(drop=True)\n",
    "\n",
    "# Combine downsampled majority class with upsampled minority class\n",
    "df_balance = pd.concat([df_0_upsampled, df_1_upsampled, df_2_upsampled, df_3_upsampled, df_4_upsampled,\n",
    "                        df_5_upsampled, df_7_upsampled, df_6_downsampled, df_8_downsampled, df_9_downsampled,\n",
    "                        df_10_downsampled]).reset_index(drop=True)\n",
    "\n",
    "# Display new class counts\n",
    "df_balance.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d4a5c",
   "metadata": {},
   "source": [
    "### Supervised Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedddef2",
   "metadata": {},
   "source": [
    "Since we already have a train and test df, there is no need to make a train test spli. We just need to separate the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split\n",
    "X = df_balance.drop('Class',axis = 1,)\n",
    "y = df_balance['Class']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scalling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e818ea8",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4918516",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "acc = round(accuracy_score(y_test, lr_pred)*100,2)\n",
    "print(\"Accurracy Score: \" + str(acc) + \"%\")\n",
    "print('-----------------------------------')\n",
    "print('Classification Report: \\n\\n' + classification_report(y_test,lr_pred, zero_division = True))\n",
    "print('----------------------------------- \\n' + 'Confusion Matrix: \\n')\n",
    "print(sns.heatmap(confusion_matrix(y_test,lr_pred), annot=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8fb60",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3198c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n_neighbors in [3,5,10,15,20,30]:\n",
    "    knn = KNeighborsClassifier(n_neighbors, weights='distance')\n",
    "    knn.fit(X_train, y_train) \n",
    "    scr = knn.score(X_test, y_test)\n",
    "    print(\"For n_neighbors = \", n_neighbors  ,\", score: \",round(scr*100,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4be101",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "knn_fit = knn.fit(X_train,y_train)\n",
    "knn_pred = knn_fit.predict(X_test)\n",
    "acc = round(accuracy_score(y_test, knn_pred)*100,2)\n",
    "print(\"Accurracy Score: \" + str(acc) + \"%\")\n",
    "print('-----------------------------------')\n",
    "print('Classification Report: \\n\\n' + classification_report(y_test,knn_pred, zero_division = True))\n",
    "print('----------------------------------- \\n' + 'Confusion Matrix: \\n')\n",
    "print(sns.heatmap(confusion_matrix(y_test,knn_pred), annot=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f99987",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ccfcc",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train,y_train)\n",
    "dtc_pred = dtc.predict(X_test)\n",
    "acc = round(accuracy_score(y_test, dtc_pred)*100,2)\n",
    "print(\"Accurracy Score: \" + str(acc) + \"%\")\n",
    "print('-----------------------------------')\n",
    "print('Classification Report: \\n\\n' + classification_report(y_test,dtc_pred, zero_division = True))\n",
    "print('----------------------------------- \\n' + 'Confusion Matrix: \\n')\n",
    "print(sns.heatmap(confusion_matrix(y_test,dtc_pred), annot=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2864d84d",
   "metadata": {},
   "source": [
    "#### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1412678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train,y_train)\n",
    "gbc_pred = gbc.predict(X_test)\n",
    "acc = round(accuracy_score(y_test, gbc_pred)*100,2)\n",
    "print(\"Accurracy Score: \" + str(acc) + \"%\")\n",
    "print('-----------------------------------')\n",
    "print('Classification Report: \\n\\n' + classification_report(y_test,gbc_pred, zero_division = True))\n",
    "print('----------------------------------- \\n' + 'Confusion Matrix: \\n')\n",
    "print(sns.heatmap(confusion_matrix(y_test,gbc_pred), annot=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad4f71",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "* Highly skewed data make realy dificult to get better results\n",
    "* Balance sampling really boosted algorithims results\n",
    "* Results between 50% and 60%, with Gradient Boosting Classifier as best with over 68%%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
